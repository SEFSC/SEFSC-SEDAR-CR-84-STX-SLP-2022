---
title: "Model Development"
---

Stock Synthesis models were initially configured using an annual catch time series and compositions that were aggregated across the available years for each source of length data. Model development proceeded stepwise from the simplest configuration to those of moderate complexity. Those sequential steps included the inclusion of the index of abundance and annual fishery-independent length compositions.

## SEDAR 84 Model Development Overview {#sec-methods-model-overview}

Table x provides the short hand naming conventions used for SEDAR 84 assessments of parrot fish in St. Croix and Yellow tail Snapper, *Ocyurus chrysurus*, in Puerto Rico as well as St. Thomas and St. John. @tbl-overview describes the model development process starting with the simplest data-limited configuration, followed by exploring data-moderate configurations, individually and combined.

The simplest configurations utilized aggregated length compositions by implementing the "super-period" approach in Stock Synthesis. When using super-periods, the estimation model generates annual values, but the likelihood function will compare the expected composite to the data composite across the super-period. When using this approach on the size composition data, Stock Synthesis models will still aim to identify parameter values for selectivity that achieve a fit between the predicted and observed data.

The data-moderate considerations explored in SEDAR 84 included incorporating (a) indices, (b) annual fishery-independent size composition, (c) annual fishery-dependent size composition, (d) recruitment deviations, (e) dome-shaped selectivity, and (f) timeblocks. Not all of these considerations were explored for St. Croix Stoplight Parrotfish. However, the process sets us a draft reusable workflow across species and islands of model development with sequential model steps that take into account methodological and intentional stepwise model explorations combining across individual complexity considerations.

| Model Development Process | Code | Sequential modeling steps |
|---------------------------|:--:|-----------------------------|
| Data-limited configuration | null | catch and super-year size data |
| Data-moderate consideration | a | index |
| Data-moderate consideration | b | annual fishery-independent size data |
| Data-moderate consideration | c | annual fishery-dependent size data |
| Data-moderate consideration | d | recruitment deviations |
| Data-moderate consideration | e | dome-shaped selectivity |
| Data-moderate consideration | f | timeblocks |
| Version combining complexity steps | v1 | a + b |
| Versions combining complexity steps | v2 | a + c |
| Versions combining complexity steps | v3 | a + d |
| Versions combining complexity steps | v4 | a + e |
| Versions combining complexity steps | v5 | a + f |
| Versions combining complexity steps | v6 | a + b + c |
| Versions combining complexity steps | v7 | a + b + d |
| Versions combining complexity steps | v8 | a + b + e |
| Versions combining complexity steps | v9 | a + b + f |
| Versions combining complexity steps | v10 | a + c + d |
| Versions combining complexity steps | v11 | a + c + e |
| Versions combining complexity steps | v12 | a + c + f |
| Versions combining complexity steps | v13 | a + d + e |
| Versions combining complexity steps | v14 | a + d + f |
| Versions combining complexity steps | v15 | a + e + f |
| Versions combining complexity steps | v16 | a + b + c + d |
| Versions combining complexity steps | v17 | a + b + c + e |
| Versions combining complexity steps | v18 | a + b + c + f |
| Versions combining complexity steps | v19 | a + b + d + e |
| Versions combining complexity steps | v20 | a + b + d + f |
| Versions combining complexity steps | v21 | a + b + e + f |
| Versions combining complexity steps | v22 | a + b + c + d + e |
| Versions combining complexity steps | v23 | a + b + c + d + f |
| Versions combining complexity steps | v24 | a + b + c + e + f |
| Versions combining complexity steps | v25 | a + b + c + d + e + f |

:  Summary of process and naming conventions used for SEDAR 84 assessments describing  model development noting the simplest data-limited configuration, exploring individual data-moderate considerations, individually and combined.  {#tbl-overview}

## SEDAR 84 St. Croix Stoplight Parrotfish Model Development {#sec-methods-model-key}

The considerations detailed in @tbl-overview that were relevant for the St. Croix stoplight parrotfish assessment were (a) indices, (b) annual fishery-independent size composition, and (d) recruitment deviations. Annual fishery-dependent size data (c) was not explored due to low sample size. Dome-shaped selectivity (e) and selectivity related timeblocks (f) were also not considerations that factor into the dynamics and data reviewed for the stoplight parrotfish population or fleet dynamics. The scenarios documented in this report are listed in table @tbl-ss3-stxslp

| Model Development Process | Code | Sequential modeling steps |
|---------------------------|:--:|-----------------------------|
| Data bin range for scenario inputs |	\_5cm_010641\_	| From 01 to 41 centimeters, 5 cm bins; First bin is 01 to 10 cm |
| Model building initial steps	| \_ct	| model initialized with continuum tool (ct) scenario inputs |
| Model building initial steps	| \_m1	| \_ct modified for biology (adjust growth + hermaphroditism) |
| Model building initial steps	| \_m2	| \_m1 modified to include continuous recruitment (four settlement events) |
| Scenario	| a_m2	| index + hermaphroditism + continuous recruitment |
| Scenario	| b_m2	| annual comp FI + hermaphroditism + continuous recruitment |
| Scenario	| v1_m2	| index + annual comp FI + hermaphroditism + continuous recruitment |
| Scenario	| v3_m2	| index + recruitment deviations + hermaphroditism  + continuous recruitment |
| Scenario	| v7_m2	| index + annual comp FI + recruitment deviations + hermaphroditism + continuous recruitment |

:  Summary of process and naming conventions used for SEDAR 84 St. Croix Stoplight Parrotfish model development.  {#tbl-ss3-stxslp}

## Model Diagnostics {#sec-methods-diagnostics}

Model diagnostics aimed to follow the conceptual process described by @carvalho2021. This process flows along axis of diagnostics types evaluating goodness of fit, information sources and structure, prediction skill, convergence, and model plausibility. Although @carvalho2021 advises detours for additional model explorations when initial diagnostic tests fail, advanced diagnostics, such as likelihood profiles, retrospective analyses and jitter analyses, were conducted even when initial tests failed to comprehensively communicate the various model configurations explored to the extent possible.  

### Convergence {#sec-methods-convergence}

Three approaches were used to check for model convergence. They were investigating for the presence of (1) bounded parameters, (2) high final gradients, and (3) a positive definite hessian. As described by @carvalho2021 checking for bounded parameters can indicate discrepancies with data or model structure. Additionally, small final gradients and a positive definite hessian can indicate that the objective function achieved good convergence.

### Correlation Analysis {#sec-methods-corr}

High correlation among parameters can lead to flat response surfaces and poor model stability. By performing a correlation analysis, modeling assumptions that lead to inadequate configurations can be identified. Because of the highly parameterized nature of stock assessment models, some parameters are expected to be correlated (e.g., stock recruit parameters). However, many strongly correlated parameters (e.g., > 0.95) suggest reconsidering modeling assumptions and parameterization.

### Evaluating Variance

Parameters with high variance do not influence the fit to the data. To check for parameters with high variance, all parameter estimates are reported with their resulting standard deviations. 

### Residual Analysis {#sec-methods-residuals}

The primary approach to address performance was a residual analysis of model fit to each data set (e.g., catch, length compositions, indices). Any temporal trend in model residuals or disproportionately high residual values can indicate model misspecification and poor performance. Ideally, residuals are randomly distributed, conform to the assumed error structure for that data source, and are not of extreme magnitude. Any extreme positive or negative residual patterns indicate poor model performance and potential unaccounted-for process or observation error.

### Jitter Analysis {#sec-methods-jitter}

Jitter analysis is a relatively simple method that can be used to assess model stability and to determine whether the search algorithm has found a global, as opposed to local, solution. The premise is that all starting values are randomly altered (or ‘jittered’) by an input constant value, and the model is rerun from the new starting values. If the resulting population trajectories across many runs converge to the same solution, this provides reasonable support that a global minimum has been obtained. This process is not fault-proof; no guarantee can ever be made that the ‘true’ solution has been found or that the model does not contain misspecification. However, if the jitter analysis results are consistent, it provides additional support that the model is performing well and has come to a stable solution. For this assessment, a jitter value 0.2 was applied to the starting values, and 30 runs were completed. The jitter value defines a uniform distribution in cumulative normal space to generate new initial parameter values [@methot2020].

### Retrospective Analysis {#sec-methods-retro}

A retrospective analysis is a helpful approach for addressing the consistency of terminal year model estimates (e.g., SSB, Recruits, Fs) and is often considered a sensitivity exploration of impacts on key parameters from changes in data. The analysis sequentially removes a year of data at a time and reruns the model. Suppose the resulting estimates of derived quantities such as SSB or recruitment differ significantly. In such a case, serial over- or underestimation of important quantities can indicate that the model has some unidentified process error and could require reassessing model assumptions. It is expected that removing data will lead to slight differences between the new terminal year estimates and the estimates for that year in the model with the complete data. Estimates in years before the terminal year may have increasingly reliable information on cohort strength. Therefore, slight differences are usually expected between model runs as more years of size composition data are sequentially removed. Ideally, the difference in estimates will be slight and more or less randomly distributed above and below the estimates from the model with the complete data sets. A five-year retrospective analysis was carried out.

### Profile Likelihoods {#sec-methods-profiles}

Profile likelihoods are used to examine the change in negative log-likelihood for each data source to address the stability of a given parameter estimate and to see how each data source influences the estimate. The analysis is performed by holding a given parameter at a constant value and rerunning the model. The model is run repeatedly over a range of reasonable parameter values. Ideally, the graph of change in likelihood values against parameter values will yield a well-defined minimum. When the profile plot shows conflicting signals or is flat across its range, the given parameter may be poorly estimated.

Typically, profiling is carried out for key parameters, particularly those defining the stock-recruit relationship (steepness, virgin recruitment, and sigma R). Profiles were explored across initial equilibrium catch, steepness, and virgin recruitment (R0).

### Sensitivity Runs {#sec-methods-sensitivity}

Sensitivity analyses were considered to evaluate the impact on key derived quantities. Sensitivities included considering alternatives for the CV associated with catch, the method used to model hermaphroditism, and data weighting. Although the data-limited implementation of SS3 will inherently nearly exactly fit the annual landings, a higher CV of 0.3 was explored via sensitivity analysis. The second method for parameterizing hermaphroditism involved using a female-only model and accounting for sex transition to males as a reduction in fecundity. Lastly, the Dirichlet multinomial approach was used to reweigh the composition data. This method allows an internal estimation of sampling variance for each source of length composition data and adjusts the effective sample sizes [@methot2013].
